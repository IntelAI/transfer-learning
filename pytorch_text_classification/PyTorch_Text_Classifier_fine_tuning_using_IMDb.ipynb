{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65f0c82",
   "metadata": {},
   "source": [
    "# Text Classifier fine tuning using IMDb with PyTorch\n",
    "\n",
    "This notebook demonstrates fine tuning pretrained models from [Hugging Face](https://huggingface.co) using the [IMDb dataset](https://huggingface.co/datasets/imdb) to analyze the sentiment of movie reviews text. The full dataset has 25,0000 training and 25,000 test examples, but this notebook uses a subset of the dataset for quicker training. The notebook uses [IntelÂ® Extension for PyTorch*](https://github.com/intel/intel-extension-for-pytorch), which extends PyTorch with optimizations for an extra performance boost on Intel hardware.\n",
    "\n",
    "Please install the dependencies from the [requirements.txt](requirements.txt) file before executing this notebook.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Get the model and setup the Trainer](#3.-Get-the-model-and-setup-the-Trainer)\n",
    "4. [Fine tuning and evaluation](#4.-Fine-tuning-and-evaluation)\n",
    "5. [Export the model](#5.-Export-the-model)\n",
    "6. [Reload the model and make predictions](#6.-Reload-the-model-and-make-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a6685",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import logging as datasets_logging\n",
    "from transformers.utils import logging as transformers_logging\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Set the logging stream to stdout\n",
    "for handler in transformers_logging._get_library_root_logger().handlers:\n",
    "    handler.setStream(sys.stdout)\n",
    "\n",
    "sh = datasets_logging.logging.StreamHandler(sys.stdout)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the Hugging Face pretrained model to use (https://huggingface.co/models)\n",
    "# For example: \n",
    "#   albert-base-v2\n",
    "#   bert-base-uncased\n",
    "#   distilbert-base-uncased\n",
    "#   distilbert-base-uncased-finetuned-sst-2-english\n",
    "#   roberta-base\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Name of the Hugging Face dataset\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# Define an output directory\n",
    "output_dir = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"{}-{}-output\".format(model_name, dataset_name))\n",
    "\n",
    "# Define a cache directory for dataset and pretrained model files\n",
    "cache_dir = os.environ[\"CACHE_DIR\"] if \"CACHE_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"pytorch-text-classification-cache\")\n",
    "\n",
    "print(\"Model name:\", model_name)\n",
    "print(\"Dataset name:\", dataset_name)\n",
    "print(\"Output directory:\", output_dir)\n",
    "print(\"Cache directory:\", cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f258d4f",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The notebook gets the [IMDb movie review dataset](https://huggingface.co/datasets/imdb) using the Hugging Face datasets API. If the notebook is executed multiple times, the dataset will be used from the cache directory, to speed up the timet that it takes to run.\n",
    "\n",
    "The IMDb dataset in Hugging Face has 3 splits: `train`, `test`, and `unsupervised`. This notebook will be using data from the `train` split for training and data from the `test` split for evaluation. The data has 2 columns: `text` (string with the movie review) and `label` (integer class label). The code in the next cell is setup to run using the IMDb dataset, so note that if a different dataset is being used, you may need to change the split names and/or the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For quicker training and debug runs, use a subset of the dataset by specifying the size of the train/eval datasets.\n",
    "# Set the sizes `None` to use the full dataset. The full IMDb dataset has 25,000 training and 25,000 test examples.\n",
    "train_dataset_size = 1000\n",
    "eval_dataset_size = 1000\n",
    "\n",
    "# Name of the dataset splits (the split names may vary if you are not using the IMDb dataset)\n",
    "train_split_name = \"train\"\n",
    "eval_split_name = \"test\"\n",
    "\n",
    "# Name of the columns in the dataset (the column names may vary if you are not using the IMDb dataset)\n",
    "dataset_sentence1_key = \"text\"\n",
    "dataset_sentence2_key = None\n",
    "dataset_label_key = \"label\"\n",
    "\n",
    "datasets_logging.set_verbosity_error()\n",
    "\n",
    "# Load the dataset from the Hugging Face dataset API\n",
    "dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Define the tokenizer args, depending on if the data has 2 sentences or just 1\n",
    "    args = ((examples[dataset_sentence1_key],) if dataset_sentence2_key is None \\\n",
    "             else (examples[dataset_sentence1_key], examples[dataset_sentence2_key]))\n",
    "    return tokenizer(*args, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the raw text from the tokenized dataset\n",
    "raw_text_columns = [dataset_sentence1_key, dataset_sentence2_key] if dataset_sentence2_key else [dataset_sentence1_key]\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(raw_text_columns)\n",
    "\n",
    "# Get the training and eval dataset based on the specified dataset sizes\n",
    "train_dataset = tokenized_dataset[train_split_name].shuffle().select(range(train_dataset_size)) if train_dataset_size \\\n",
    "    else tokenized_dataset[train_split_name]    \n",
    "eval_dataset = tokenized_dataset[eval_split_name].shuffle().select(range(eval_dataset_size)) if eval_dataset_size \\\n",
    "    else tokenized_dataset[eval_split_name]\n",
    "\n",
    "# Save the class label information to use later when predicting\n",
    "class_labels = dataset[train_split_name].features[dataset_label_key]\n",
    "print(\"Label names:\", class_labels.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f6a56",
   "metadata": {},
   "source": [
    "The next cell displays a sample of the text and labels so that we can see what our training data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4301f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of the dataset to display\n",
    "sample_size = 7\n",
    "sentence1_sample = dataset[train_split_name][dataset_sentence1_key][:sample_size]\n",
    "sentence2_sample = dataset[train_split_name][dataset_sentence2_key][:sample_size] if dataset_sentence2_key else None\n",
    "label_sample = dataset[train_split_name][dataset_label_key][:sample_size]\n",
    "dataset_sample = zip(sentence1_sample, sentence2_sample, label_sample) if dataset_sentence2_key \\\n",
    "    else zip(sentence1_sample, label_sample)\n",
    "\n",
    "columns = [dataset_sentence1_key, dataset_sentence2_key, dataset_label_key] if dataset_sentence2_key else \\\n",
    "    [dataset_sentence1_key, dataset_label_key]\n",
    "\n",
    "# Display the sample using a dataframe\n",
    "sample = pd.DataFrame(dataset_sample, columns=columns)\n",
    "sample.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a24bcd",
   "metadata": {},
   "source": [
    "## 3. Get the model and setup the Trainer\n",
    "\n",
    "This step gets the pretrained model from [Hugging Face](https://huggingface.co/models) and sets up the\n",
    "[TrainingArguments](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments) and the\n",
    "[Trainer](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer). For simplicity, this example is using default values for most of the training args, but we are specifying our output directory and the number of training epochs. If your output directory already has checkpoints from a previous run,\n",
    "training will resume from the last checkpoint. The `overwrite_output_dir` training argument can be set to\n",
    "`True` if you want to instead overwrite previously generated checkpoints.\n",
    "\n",
    "> Note that it is expected to see a warning at this step about some weights not being used. This is because\n",
    "> the pretraining head from the original model is being replaced with a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797485aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "\n",
    "# Load the model using the pretrained weights\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(class_labels.names))\n",
    "\n",
    "# Apply the ipex optimize function to the model\n",
    "model = ipex.optimize(model)\n",
    "\n",
    "# Set up the training args\n",
    "training_args = TrainingArguments(output_dir=output_dir, num_train_epochs=num_train_epochs)\n",
    "\n",
    "# Compute metrics used for evaluation\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=eval_dataset,\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3929cd1",
   "metadata": {},
   "source": [
    "## 4. Fine tuning and evaluation\n",
    "\n",
    "This step uses the [Trainer](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer)\n",
    "defined in the previous step to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ae450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print metrics\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "for key in metrics.keys():\n",
    "    print(\"{}: {}\".format(key, metrics[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b873f",
   "metadata": {},
   "source": [
    "## 5. Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to our output directory\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49449342",
   "metadata": {},
   "source": [
    "## 6. Reload the model and make predictions\n",
    "\n",
    "The output directory is used to reload the model. In the next cell, we evalute the reloaded model to verify that we are getting the same metrics that we saw after fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "reloaded_model.eval()\n",
    "\n",
    "# Use the reloaded model in the trainer\n",
    "trainer.model = reloaded_model\n",
    "reloaded_model_metrics = trainer.evaluate()\n",
    "\n",
    "for key in metrics.keys():\n",
    "    print(\"{}: {}\".format(key, reloaded_model_metrics[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a5386",
   "metadata": {},
   "source": [
    "Next, we demonstrate how encode raw text input and get predictions from the reloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d231c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some raw text input\n",
    "raw_text_input = [\"It was okay. I finished it, but wouldn't watch it again.\",\n",
    "                  \"So bad\",\n",
    "                  \"Definitely not my favorite\",\n",
    "                  \"Highly recommended\"]\n",
    "\n",
    "# Encode the raw text using the tokenizer\n",
    "encoded_input = tokenizer(raw_text_input, padding=True, return_tensors='pt')\n",
    "\n",
    "# Send the encoded input to the model and get the predicted results\n",
    "output = model(**encoded_input)\n",
    "_, predictions = torch.max(output.logits, dim=1)\n",
    "\n",
    "# Translate the predictions to class label strings\n",
    "prediction_labels = class_labels.int2str(predictions)\n",
    "\n",
    "# Create a dataframe to display the results\n",
    "result_list = [list(x) for x in zip(raw_text_input, prediction_labels)]\n",
    "result_df = pd.DataFrame(result_list, columns=[\"Input Text\", \"Predicted Label\"])\n",
    "result_df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee40324",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
