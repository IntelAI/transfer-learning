{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary text classification using BERT models from TF Hub\n",
    "\n",
    "This notebook demonstrates fine tuning BERT models from [TF Hub](https://tfhub.dev) with the [IMDb movie review dataset from TensorFlow datasets](https://www.tensorflow.org/datasets/catalog/imdb_reviews) for sentiment analysis.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Install dependencies and setup parameters](#1.-Install-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Build the model](#3.-Build-the-model)\n",
    "4. [Fine tuning and evaluation](#4.-Fine-tuning-and-evaluation)\n",
    "5. [Export the model](#5.-Export-the-model)\n",
    "6. [Reload the model and make predictions](#6.-Reload-the-model-and-make-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies and setup parameters\n",
    "\n",
    "The notebook assumes that you have already followed the README.md instructions that install Intel-optimized TensorFlow or use the Intel-optimized TensorFlow jupyter docker container. Additional installations needed to run the notebook are done in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pip\n",
    "!pip install -q ipywidgets==7.6.5 \\\n",
    "                tensorflow-hub==0.12.0 \\\n",
    "                tensorflow-datasets==4.5.2 \\\n",
    "                'pandas>=1.1.5'\n",
    "!pip install --no-deps -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Note that tensorflow_text isn't used directly but the import is required to register ops used by the\n",
    "# BERT text preprocessor\n",
    "import tensorflow_text\n",
    "\n",
    "from bert_utils import get_model_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run one of the supported [BERT models from TF Hub](https://tfhub.dev/google/collections/bert/1). The table below has a list of the available models and links to their URLs in TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF Hub model map from json and print a list of the supported models\n",
    "tfhub_model_map, models_df = get_model_map(\"tfhub_bert_model_map_classifier.json\", return_data_frame=True)\n",
    "models_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the BERT model to use. This string must match one of the models listed in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"small_bert/bert_en_uncased_L-2_H-128_A-2\"\n",
    "if model_name not in tfhub_model_map.keys():\n",
    "    raise ValueError(\"The specified model name ({}) is not supported\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a working directory where the dataset will be downloaded\n",
    "if \"WORKING_DIR\" in os.environ and os.environ[\"WORKING_DIR\"] != \"\":\n",
    "    working_dir = os.environ[\"WORKING_DIR\"]\n",
    "else:\n",
    "    working_dir = input(\"Path to a working directory (to download datasets): \")\n",
    "\n",
    "# Define an output directory for the saved model to be exported\n",
    "if \"OUTPUT_DIR\" in os.environ and os.environ[\"OUTPUT_DIR\"] != \"\":\n",
    "    output_dir = os.environ[\"OUTPUT_DIR\"]\n",
    "else:\n",
    "    output_dir = input(\"Path to an output directory (for the saved model): \")\n",
    "\n",
    "# Output directory for logs and checkpoints generated during training\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "tfhub_preprocess = tfhub_model_map[model_name][\"preprocess\"]\n",
    "tfhub_bert_encoder = tfhub_model_map[model_name][\"bert_encoder\"]\n",
    "\n",
    "print(\"Using TF Hub model:\", model_name)\n",
    "print(\"BERT encoder URL:\", tfhub_bert_encoder)\n",
    "print(\"Preprocessor URL:\", tfhub_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "Load the dataset using the TensorFlow datasets library and get splits for training, validation, and test that were defined earlier. The [tfds.load()](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) function will download the dataset if it's not found in the dataset directory. Subsequent runs will reuse the dataset that was downloaded the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configs used for TensorFlow datasets\n",
    "tfds_config = {\n",
    "    # Name of the tensorflow dataset to use (ex: imdb_reviews)\n",
    "    # For a full list of options see: https://www.tensorflow.org/datasets/catalog/overview\n",
    "    \"tfds_name\": \"imdb_reviews\",\n",
    "    # Define the splits used for the train, validation, and test dataset\n",
    "    # A larger amount of training data can result in better accuracy, but will have a longer training time\n",
    "    \"train_split\": \"train[:50%]\",\n",
    "    \"val_split\": \"train[:20%]\",\n",
    "    \"test_split\": \"test[:20%]\"\n",
    "}\n",
    "\n",
    "# Training batch size\n",
    "batch_size = 32\n",
    "\n",
    "def get_dataset_from_tfds(dataset_dir, configs):\n",
    "    return tfds.load(configs[\"tfds_name\"],\n",
    "                     data_dir=dataset_dir,\n",
    "                     split=[configs[\"train_split\"], configs[\"val_split\"], configs[\"test_split\"]],\n",
    "                     batch_size=batch_size,\n",
    "                     as_supervised=True,\n",
    "                     shuffle_files=True,\n",
    "                     with_info=True)\n",
    "\n",
    "# Location where the dataset will be downloaded\n",
    "dataset_dir = os.path.join(output_dir, tfds_config[\"tfds_name\"])\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "[train_ds, val_ds, test_ds], info = get_dataset_from_tfds(dataset_dir, tfds_config)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model\n",
    "\n",
    "Create the BERT model to fine tune using an input layer, the preprocessing layer (from TF Hub), the BERT encoder layer (from TF Hub), one dense layer, and a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input_layer')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(input_layer)\n",
    "encoder_layer = hub.KerasLayer(tfhub_bert_encoder, trainable=True, name='encoder')\n",
    "outputs = encoder_layer(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "net = tf.keras.layers.Dropout(0.1)(net)\n",
    "net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "classifier_model = tf.keras.Model(input_layer, net)\n",
    "\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning and evaluation\n",
    "\n",
    "Train the model for the specified number of epochs, then evaluate the model using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The number of training epochs to run\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Maximum total input sequence length after WordPiece tokenization (longer sequences will be truncated)\n",
    "max_seq_length = 128\n",
    "\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08),\n",
    "                         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics=tf.metrics.BinaryAccuracy())\n",
    "\n",
    "history = classifier_model.fit(train_ds, validation_data=val_ds, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy using the test dataset. If the accuracy does not meet your expectations, try to increasing the size of the training dataset split or the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export the model\n",
    "\n",
    "Since training has completed, export the `saved_model.pb` to the output directory in a folder with the model and dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"{}_{}\".format(model_name, tfds_config[\"tfds_name\"])\n",
    "model_dir = os.path.join(output_dir, model_dir)\n",
    "classifier_model.save(model_dir, include_optimizer=False)\n",
    "\n",
    "saved_model_path = os.path.join(model_dir, \"saved_model.pb\")\n",
    "if os.path.exists(saved_model_path):\n",
    "    print(\"Saved model location:\", saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reload the model and make predictions\n",
    "\n",
    "Reload from the `saved_model.pb` in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines a list of strings to send as input to the reloaded model. If you are using a dataset other than the [IMDB movie reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews), you can update the snippet below with your own list of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tfds_config[\"tfds_name\"] == \"imdb_reviews\":\n",
    "    input_text = [\"Awesome movie\",\n",
    "                  \"It was entertaining, but completely predictable.\",\n",
    "                  \"Wasn't what I expected, but I still enjoyed it\",\n",
    "                  \"I wouldn't recommend this movie to my worst enemy\",\n",
    "                  \"I'm not sure how good the movie was, because I fell asleep\"]\n",
    "else:\n",
    "    # Define your own list of input text\n",
    "    input_text = []\n",
    "    \n",
    "if not input_text:\n",
    "    raise ValueError(\"Please define the list of input_text strings.\")\n",
    "    \n",
    "predict_results = tf.sigmoid(reloaded_model(tf.constant(input_text)))\n",
    "\n",
    "result_list = [[input_text[i], tf.get_static_value(predict_results[i])[0]] for i in range(len(input_text))]\n",
    "result_df = pd.DataFrame(result_list, columns=[\"Input Text\", \"Score\"])\n",
    "result_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
