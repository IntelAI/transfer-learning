{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass text classification using BERT models from TF Hub\n",
    "\n",
    "This notebook demonstrates fine tuning BERT models from [TF Hub](https://tfhub.dev) with multiclass text classification datasets.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Build the model](#3.-Build-the-model)\n",
    "4. [Fine tuning and evaluation](#4.-Fine-tuning-and-evaluation)\n",
    "5. [Export the model](#5.-Export-the-model)\n",
    "6. [Reload the model and make predictions](#6.-Reload-the-model-and-make-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [README.md](/notebooks/README.md) to setup a TensorFlow environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Note that tensorflow_text isn't used directly but the import is required to register ops used by the\n",
    "# BERT text preprocessor\n",
    "import tensorflow_text\n",
    "\n",
    "from bert_utils import get_model_map\n",
    "from tlt.utils.file_utils import download_and_extract_zip_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run one of the supported [BERT models from TF Hub](https://tfhub.dev/google/collections/bert/1). The table below has a list of the available models and links to their URLs in TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF Hub model map from json and print a list of the supported models\n",
    "tfhub_model_map, models_df = get_model_map(\"tfhub_bert_model_map_classifier.json\", return_data_frame=True)\n",
    "models_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the BERT model to use. This string must match one of the models listed in the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"small_bert/bert_en_uncased_L-2_H-128_A-2\"\n",
    "if model_name not in tfhub_model_map.keys():\n",
    "    raise ValueError(\"The specified model name ({}) is not supported\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory to download the dataset\n",
    "dataset_directory = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "\n",
    "# Define an output directory for the saved model to be exported\n",
    "output_directory = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "# Output directory for logs and checkpoints generated during training\n",
    "if not os.path.isdir(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "tfhub_preprocess = tfhub_model_map[model_name][\"preprocess\"]\n",
    "tfhub_bert_encoder = tfhub_model_map[model_name][\"bert_encoder\"]\n",
    "\n",
    "print(\"Using TF Hub model:\", model_name)\n",
    "print(\"BERT encoder URL:\", tfhub_bert_encoder)\n",
    "print(\"Preprocessor URL:\", tfhub_preprocess)\n",
    "print(\"Dataset directory:\", dataset_directory)\n",
    "print(\"Output directory:\", output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The notebook gets the  dataset from the [TensorFlow Datasets catalog](https://www.tensorflow.org/datasets/catalog/overview)\n",
    "\n",
    "The code ends up defining [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects for each split (train, validation, and test) and a map for the translating the numerical to string label.\n",
    "\n",
    "Execute the following cell to set the batch size and declare the base class used for the dataset setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Base class used for defining the multi text classification dataset being used\n",
    "class MultiTextClassificationData():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.label_map = label_map\n",
    "        self.reverse_label_map = {}\n",
    "        self.train_ds = None\n",
    "        self.val_ds = None\n",
    "        self.test_ds = None\n",
    "        self.dataset_name = \"\"\n",
    "        \n",
    "        for k, v in self.label_map.items():\n",
    "            self.reverse_label_map[v] = k\n",
    "        \n",
    "    def get_str_label(self, numerical_value):\n",
    "        if not isinstance(numerical_value, int):\n",
    "            numerical_value = int(tf.math.round(numerical_value))\n",
    "        \n",
    "        if numerical_value in self.label_map.keys():\n",
    "            return label_map[numerical_value]\n",
    "        else:\n",
    "            raise ValueError(\"The key {} was not found in the label map\".format(numerical_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a TensorFlow dataset\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets) has a [catalog of datasets](https://www.tensorflow.org/datasets/catalog/overview) that can be specified by name. Information about the dataset is available in the catalog (including information on the size of the dataset and the splits).\n",
    "\n",
    "The next cell demonstrates using the [`ag_news_subset`](https://www.tensorflow.org/datasets/catalog/ag_news_subset) dataset from the TensorFlow datasets catalog to get splits for training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFDSMultiTextClassificationData(MultiTextClassificationData):\n",
    "    def __init__(self, dataset_dir, tfds_name, train_split, val_split, test_split, batch_size):\n",
    "        \"\"\"\n",
    "        Intialize the TFDSMultiTextClassificationData class for a dataset Multi text classification dataset\n",
    "        from the TensorFlow dataset catalog.\n",
    "        \n",
    "        :param dataset_dir: Path to a dataset directory to read/write data\n",
    "        :param tfds_name: String name of the TensorFlow dataset to load\n",
    "        :param train_split: String specifying which split to load for training (e.g. \"train[:80%]\"). See the\n",
    "                            https://www.tensorflow.org/datasets/splits documentation for more information on\n",
    "                            defining splits.\n",
    "        :param val_split: String specifying the split to load for validation.\n",
    "        :param test_split: String specifying the split to load for test.\n",
    "        :param batch_size: Batch size\n",
    "        \"\"\"\n",
    "        # Init base class\n",
    "        MultiTextClassificationData.__init__(self, batch_size) \n",
    "        \n",
    "        [self.train_ds, self.val_ds, self.test_ds], info = tfds.load(tfds_name,\n",
    "                     data_dir=dataset_dir,\n",
    "                     split=[train_split, val_split, test_split],\n",
    "                     batch_size=batch_size,\n",
    "                     as_supervised=True,\n",
    "                     shuffle_files=True,\n",
    "                     with_info=True)\n",
    "        self.dataset_name = tfds_name\n",
    "        print(info)\n",
    "\n",
    "\n",
    "# Name of the TFDS to use\n",
    "tfds_name=\"ag_news_subset\"\n",
    "\n",
    "# Location where the dataset will be downloaded\n",
    "dataset_directory = os.path.join(dataset_directory, tfds_name)\n",
    "if not os.path.isdir(dataset_directory):\n",
    "    os.makedirs(dataset_directory)\n",
    "\n",
    "# Label map for sentiment analysis\n",
    "label_map = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\",\n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "    \n",
    "# Initialize the dataset splits using a dataset from the TensorFlow datasets catalog\n",
    "dataset = TFDSMultiTextClassificationData(dataset_dir=dataset_directory,\n",
    "                                           tfds_name=tfds_name,\n",
    "                                           train_split=\"train[:50%]\",\n",
    "                                           val_split=\"train[:20%]\",\n",
    "                                           test_split=\"test[:20%]\",\n",
    "                                           batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the model\n",
    "\n",
    "Create the BERT model to fine tune using a input layer, the preprocessing layer (from TF Hub), the BERT encoder layer (from TF Hub), one dense layer, and a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input_layer')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(input_layer)\n",
    "encoder_layer = hub.KerasLayer(tfhub_bert_encoder, trainable=True, name='encoder')\n",
    "outputs = encoder_layer(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "net = tf.keras.layers.Dropout(0.1)(net)\n",
    "net = tf.keras.layers.Dense(len(label_map), activation=None, name='classifier')(net)\n",
    "classifier_model = tf.keras.Model(input_layer, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning and evaluation\n",
    "\n",
    "Train the model for the specified number of epochs, then evaluate the model using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The number of training epochs to run\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Maximum total input sequence length after WordPiece tokenization (longer sequences will be truncated)\n",
    "max_seq_length = 128\n",
    "\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08),\n",
    "                         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                         metrics=tf.metrics.CategoricalAccuracy())\n",
    "\n",
    "history = classifier_model.fit(dataset.train_ds,\n",
    "                               validation_data=dataset.val_ds,\n",
    "                               epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy using the test dataset. If the accuracy does not meet your expectations, try to increasing the size of the training dataset split or the number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(dataset.test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using a single batch from the test dataset, and then display the results along with the input text and the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1\n",
    "predictions = classifier_model.predict(dataset.test_ds, batch_size=batch_size, steps=num_steps)\n",
    "\n",
    "prediction_list = []\n",
    "step_count = 0\n",
    "\n",
    "for batch in dataset.test_ds:\n",
    "    label_list = list(batch[1].numpy())\n",
    "    text_list = list(batch[0].numpy())\n",
    "    \n",
    "    for i, (text, actual_label) in enumerate(zip(text_list, label_list)):\n",
    "        score = tf.math.sigmoid(predictions[i])\n",
    "        score = tf.reduce_max(score)\n",
    "        prediction = int(tf.math.round(score))\n",
    "        prediction = dataset.get_str_label(prediction)\n",
    "        prediction_list.append([text.decode('utf-8'),\n",
    "                                tf.get_static_value(score),\n",
    "                                prediction,\n",
    "                                dataset.get_str_label(actual_label)])\n",
    "    \n",
    "    step_count += 1\n",
    "    if num_steps <= step_count:\n",
    "        break\n",
    "    \n",
    "result_df = pd.DataFrame(prediction_list, columns=[\"Input Text\", \"Score\", \"Predicted Label\", \"Actual Label\"])\n",
    "result_df.style.hide(axis=\"index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export the model\n",
    "\n",
    "Since training has completed, export the `saved_model.pb` to the output directory in a folder with the model and dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"{}_{}\".format(model_name, dataset.dataset_name)\n",
    "model_dir = os.path.join(output_directory, model_dir)\n",
    "classifier_model.save(model_dir, include_optimizer=False)\n",
    "\n",
    "saved_model_path = os.path.join(model_dir, \"saved_model.pb\")\n",
    "if os.path.exists(saved_model_path):\n",
    "    print(\"Saved model location:\", saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reload the model and make predictions\n",
    "\n",
    "Reload from the `saved_model.pb` in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section defines a list of strings to send as input to the reloaded model. If you are using a dataset other than the [AG News dataset](https://www.tensorflow.org/datasets/catalog/ag_news_subset), you can update the snippet below with your own list of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_text = [\"WASHINGTON - Employers stepped up hiring in August, expanding payrolls by 144,000 and lowering the unemployment rate to 5.4 percent.\",\n",
    "              \"PRESENTACION, Philippines (Reuters) - Philippine communist rebels freed Wednesday two soldiers they had held as 'prisoners of war' for'\\\n",
    "              'more than five months, saying they wanted to rebuild confidence in peace talks with the government.\", \n",
    "              \"Geneva - Worldwide sales of industrial robots surged to record levels in the first half of 2004 after equipment prices fell while labour' \\\n",
    "              'costs grew, the United Nations Economic Commission for Europe said in a report to be released today.\"]\n",
    "    \n",
    "if not input_text:\n",
    "    raise ValueError(\"Please define the list of input_text strings.\")\n",
    "\n",
    "# Send the input text to the reloaded model\n",
    "predict_results = tf.sigmoid(reloaded_model(tf.constant(input_text)))\n",
    "\n",
    "# Get the results into a data frame to display\n",
    "result_list = [[input_text[i],\n",
    "                tf.get_static_value(predict_results[i])[0],\n",
    "                dataset.get_str_label(tf.get_static_value(predict_results[i])[0])] for i in range(len(input_text))]\n",
    "result_df = pd.DataFrame(result_list, columns=[\"Input Text\", \"Score\", \"Predicted Label\"])\n",
    "result_df.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@misc{zhang2015characterlevel,\n",
    "    title={Character-level Convolutional Networks for Text Classification},\n",
    "    author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
    "    year={2015},\n",
    "    eprint={1509.01626},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tlt_notebook_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0aa4059be174cbf31be2e5e13e301e374cc50c7151267adca3135ebe59561bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
