{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a68447",
   "metadata": {},
   "source": [
    "# Transfer Learning for Video Classification\n",
    "\n",
    "This notebook uses video classification models from [torchvision](https://pytorch.org/vision/stable/index.html) that were originally trained using [Kinetics-400](https://arxiv.org/abs/1705.06950) and does transfer learning on the HMBD51 dataset.\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Predict using the original model](#3.-Predict-using-the-original-model)\n",
    "4. [Transfer Learning](#4.-Transfer-Learning)\n",
    "5. [Predict](#5.-Predict)\n",
    "6. [Export the saved model](#6.-Export-the-saved-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06a7f2",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [README.md](/notebooks/notebooks/setup.md) to setup a PyTorch environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519160e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models.video\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pydoc import locate\n",
    "import warnings\n",
    "\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tlt.utils.file_utils import download_and_extract_tar_file, download_file\n",
    "from model_utils import torchvision_model_map, get_retrainable_model\n",
    "\n",
    "from torchvision.io.video import read_video\n",
    "from torchvision.models.video import r3d_18, mc3_18, r2plus1d_18\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('Supported models:')\n",
    "print('\\n'.join(torchvision_model_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a6b5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify a model from the list above\n",
    "model_name = 'r3d_18'\n",
    "\n",
    "# Specify the the parent directory for the custom or torchvision dataset\n",
    "dataset_directory = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "    \n",
    "# Specify a directory for output\n",
    "output_directory = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\")\n",
    "\n",
    "print(\"Dataset directory:\", dataset_directory)\n",
    "print(\"Output directory:\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc637d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name not in torchvision_model_map.keys():\n",
    "    raise ValueError(\"The specified model_name ({}) is invalid. Please select from: {}\".\n",
    "                     format(model_name, torchvision_model_map.keys()))\n",
    "    \n",
    "print(\"Pretrained Video Classification Model:\", model_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a9495",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67956c77",
   "metadata": {},
   "source": [
    "We will be using the HMDB51 Action Recognition dataset. Run the cell below to download the dataset to the specified dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar --output $dataset_directory/hmdb51_org.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and specify our video directory\n",
    "os.makedirs(os.path.join(dataset_directory, 'hmdb51_org'), exist_ok=True)\n",
    "video_directory = os.path.join(dataset_directory, 'hmdb51_org')\n",
    "downloaded_directory = os.path.join(dataset_directory, 'hmdb51_org.rar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b76a4",
   "metadata": {},
   "source": [
    "Run the cell below to extract the .rar files and organize the HMDB51 data into subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178dce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract .rar files and move them into respective folders\n",
    "! unrar e $downloaded_directory $video_directory \n",
    "! rm $downloaded_directory -r\n",
    "\n",
    "for files in os.listdir(video_directory):\n",
    "    foldername = files.split('.')[0]\n",
    "    os.system(\"mkdir -p \" + os.path.join(video_directory, foldername))\n",
    "    os.system(\"unrar e \" + os.path.join(video_directory, files) + \" \" + os.path.join(video_directory, foldername))\n",
    "\n",
    "! rm $video_directory/*.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a88044",
   "metadata": {},
   "source": [
    "Optional: Uncomment and run the cell below if you would like to convert the video frames to images in a separate folder rather than overwriting the original video dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "\n",
    "# ! cp -R $video_directory 'hmdb51_jpeg'\n",
    "# video_directory = os.path.join(dataset_directory, 'hmdb51_jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4897d2",
   "metadata": {},
   "source": [
    "## Convert the Video Frames to Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fa4f4",
   "metadata": {},
   "source": [
    "In order to reduce computational complexity, we will convert the video frames to images using the functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d126411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get each video from the dataset, returns video ids, labels, and folders\n",
    "def get_vids(jpeg_path):\n",
    "    folders = os.listdir(jpeg_path)\n",
    "    ids = []\n",
    "    labels = []\n",
    "    for folder in folders:\n",
    "        folderpath = os.path.join(jpeg_path, folder)\n",
    "        files = os.listdir(folderpath)\n",
    "        filepath= [os.path.join(folderpath, file) for file in files]\n",
    "        ids.extend(filepath)\n",
    "        labels.extend([folderpath]*len(files))\n",
    "    return ids, labels, folders\n",
    "\n",
    "# For each video, return a list of n_frames frames\n",
    "def get_frames(filename, n_frames= 1):\n",
    "    frames = []\n",
    "    v_cap = cv2.VideoCapture(filename)\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_list= np.linspace(0, v_len-1, n_frames+1, dtype=np.int16)\n",
    "    \n",
    "    for fn in range(v_len):\n",
    "        success, frame = v_cap.read()\n",
    "        if success is False:\n",
    "            continue\n",
    "        if (fn in frame_list):\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "            frames.append(frame)\n",
    "    v_cap.release()\n",
    "    return frames, v_len\n",
    "\n",
    "# Convert each frame to a .jpg file\n",
    "def frames_to_jpg(frames, pic_path):\n",
    "    for index, frame in enumerate(frames):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  \n",
    "        image_path = os.path.join(pic_path, \"frame\"+str(index)+\".jpg\")\n",
    "        cv2.imwrite(image_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934b38c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use helper functions from above to get n_frames frames from each video and convert them to Images\n",
    "\n",
    "# Number of frames you would like to use per video\n",
    "n_frames = 16\n",
    "# Video dataset format\n",
    "extension = '.avi'\n",
    "\n",
    "print(\"Converting video frames to jpg...\")\n",
    "print(\"Note: This may take 15-20 minutes\")\n",
    "for root, dirs, files in tqdm(os.walk(video_directory, topdown=False), bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}'):\n",
    "    for name in files:\n",
    "        if extension not in name:\n",
    "            continue\n",
    "        video_path = os.path.join(root, name)\n",
    "        # Get video frames\n",
    "        frames, vlen = get_frames(video_path, n_frames= n_frames)\n",
    "        pic_path = video_path.replace(extension, \"\")\n",
    "        os.makedirs(pic_path, exist_ok= True)\n",
    "        # Convert frames to jpg\n",
    "        frames_to_jpg(frames, pic_path)\n",
    "print(\"Success.\")\n",
    "        \n",
    "# Remove redundant video.{extension} files from the folder    \n",
    "for folder in os.listdir(video_directory):\n",
    "    for file in os.listdir(video_directory + '/' + folder):\n",
    "        if extension in file:\n",
    "            os.remove(video_directory + '/' + folder + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed17b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing transforms\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.Resize((112, 112)))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip())\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103e626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 30\n",
    "test_batch_size = 30\n",
    "\n",
    "# Create the dataset and DataLoader objects\n",
    "\n",
    "print(\"loading dataset...\")\n",
    "dataset = datasets.ImageFolder(video_directory, get_transform(True))\n",
    "print(\"Loaded.\")\n",
    "print(\"loading test dataset...\")\n",
    "dataset_test = datasets.ImageFolder(video_directory, get_transform(False))\n",
    "print(\"Loaded.\")\n",
    "class_names = dataset.classes\n",
    "\n",
    "# Use 25% for validation and 75% for training\n",
    "print(\"Segmenting datasets...\")\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "num_training_samples = math.floor(len(dataset)*.75)\n",
    "\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-num_training_samples:])\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:num_training_samples])\n",
    "print(\"Success.\")\n",
    "\n",
    "\n",
    "# define DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=train_batch_size, \n",
    "                                           shuffle=True, num_workers=4)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size, \n",
    "                                           shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac630cc7",
   "metadata": {},
   "source": [
    "## 3. Predict using the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4dd97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in the pretrained model\n",
    "pretrained_model_class = locate('torchvision.models.video.{}'.format(model_name))\n",
    "model = pretrained_model_class(pretrained=True)\n",
    "inputs, classes = next(iter(train_loader))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d944c7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rearrange channels and create outputs for predictions\n",
    "inputs.unsqueeze_(1)\n",
    "inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "print(\"Preparing model outputs for prediction...\")\n",
    "outputs = model(inputs)\n",
    "print(\"Model outputs created. We can now predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Kinetics400 labels for displaying with the predictions\n",
    "kinetics400_classes = []\n",
    "labels_file_url = 'https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt'\n",
    "labels_file_path = os.path.join(dataset_directory, os.path.basename(labels_file_url))\n",
    "if not os.path.exists(labels_file_url):\n",
    "    download_file(labels_file_url, dataset_directory)\n",
    "\n",
    "with open(labels_file_path) as f:\n",
    "    kinetics400_labels = f.readlines()\n",
    "    kinetics400_classes = [l.strip() for l in kinetics400_labels]\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aadd42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of the actual labels for this batch\n",
    "actual_label_batch = [class_names[int(id)] for id in classes]\n",
    "\n",
    "# Make predictions\n",
    "_, predicted_id = torch.max(outputs, 1)\n",
    "predicted_label_batch = [kinetics400_classes[id] for id in predicted_id]\n",
    "\n",
    "# Visualize predictions using pandas dataframe object\n",
    "results_table = []\n",
    "count = 0\n",
    "for prediction, actual in zip(predicted_label_batch, actual_label_batch):\n",
    "    if prediction == actual:\n",
    "        count += 1\n",
    "    results_table.append([prediction, actual])\n",
    "\n",
    "# Display predictions and accuracy\n",
    "acc = count / len(actual_label_batch)\n",
    "print(\"Batch Accuracy: \" + str(acc))\n",
    "print(\"note: some predictions may differ by single characters\")\n",
    "pd.DataFrame(results_table, columns=[\"Prediction\", \"Actual Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49832b85",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f386013",
   "metadata": {},
   "source": [
    "Replace the pretrained head of the network with a new layer based on the number of classes in our dataset. Train the model using the new dataset for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab8424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Specify batch sizes for transfer learning\n",
    "train_batch_size = 30\n",
    "test_batch_size = 30\n",
    "\n",
    "# To reduce training time, the feature extractor layer can remain frozen (do_fine_tuning=False).\n",
    "# Fine-tuning can be enabled to potentially get better accuracy. Note that enabling fine-tuning\n",
    "# will increase training time.\n",
    "do_fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1264812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(model, criterion, optimizer, dataset, dataset_test, num_epochs=1):\n",
    "    since = time.time()\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=train_batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "    data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        # Iterate over data.\n",
    "        \n",
    "        for inputs, labels in tqdm(data_loader, bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}'):\n",
    "            x = len(inputs)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward pass\n",
    "            with torch.set_grad_enabled(True):\n",
    "                inputs.unsqueeze_(1)\n",
    "                inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "                inputs = inputs.float()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataset)\n",
    "        print(f'Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "            \n",
    "        # Iterate over data.\n",
    "        for inputs, labels in tqdm(data_loader_test, bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(False):\n",
    "                inputs.unsqueeze_(1)\n",
    "                inputs = inputs.permute(0, 2, 1, 3, 4)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(dataset_test)\n",
    "        epoch_acc = running_corrects.double() / len(dataset_test)\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "        \n",
    "        print(f'Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print()\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best Validation Accuracy: {best_acc:4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa01fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_retrainable_model(model_name, len(class_names), do_fine_tuning)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "print('Trainable parameters: {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3f5ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "model = main(model, criterion, optimizer, dataset, dataset_test, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571d385",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5021e",
   "metadata": {},
   "source": [
    "Now, let's see how Transfer Learning has improved our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09233b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(inputs)\n",
    "_, predicted_id = torch.max(outputs, 1)\n",
    "predicted_label_batch = [class_names[id] for id in predicted_id]\n",
    "count = 0\n",
    "results_table = []\n",
    "for prediction, actual in zip(predicted_label_batch, actual_label_batch):\n",
    "    if prediction == actual:\n",
    "        count += 1\n",
    "    results_table.append([prediction, actual])\n",
    "\n",
    "acc = count / (len(actual_label_batch))\n",
    "print(\"Batch Accuracy: \" + str(acc))\n",
    "\n",
    "pd.DataFrame(results_table, columns=[\"Prediction\", \"Actual Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df5dff",
   "metadata": {},
   "source": [
    "## 6. Export the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9048f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "file_path = \"{}/video_classification.pt\".format(output_directory)\n",
    "torch.save(model.state_dict(), file_path)\n",
    "print(\"Saved to {}\".format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b78fd6",
   "metadata": {},
   "source": [
    "## Dataset citations\n",
    "```\n",
    "@inproceedings{\n",
    "  title = {HMDB: A Large Video Database for Human Motion Recognition},\n",
    "  author = {H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre},\n",
    "  year = {2011}\n",
    "}\n",
    "@ONLINE {HMDB,\n",
    "author = {H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre},\n",
    "title = \"HMDB51\",\n",
    "year = \"2011\",\n",
    "url = \"https://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_org.rar\" }\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
